{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58dc2d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# import visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d0912c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No.\\n(New)</th>\n",
       "      <th>Laser power</th>\n",
       "      <th>Scan speed</th>\n",
       "      <th>Line spacing</th>\n",
       "      <th>Layer thickness</th>\n",
       "      <th>Energy density</th>\n",
       "      <th>MPST-W</th>\n",
       "      <th>MPST-D</th>\n",
       "      <th>MPST-H</th>\n",
       "      <th>MPAB-W</th>\n",
       "      <th>...</th>\n",
       "      <th>TP-HD-EB</th>\n",
       "      <th>TP-HD-YS</th>\n",
       "      <th>TP-VD-UTS</th>\n",
       "      <th>TP-VD-EB</th>\n",
       "      <th>TP-VD-YS</th>\n",
       "      <th>Productivity</th>\n",
       "      <th>Deformation</th>\n",
       "      <th>DA-Overmelting</th>\n",
       "      <th>DA-Accuracy</th>\n",
       "      <th>DA-Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A2</td>\n",
       "      <td>400</td>\n",
       "      <td>400</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.04</td>\n",
       "      <td>117.924528</td>\n",
       "      <td>240.653667</td>\n",
       "      <td>264.694333</td>\n",
       "      <td>63.358667</td>\n",
       "      <td>374.69667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.133932</td>\n",
       "      <td>428.316826</td>\n",
       "      <td>1210.143161</td>\n",
       "      <td>0.161242</td>\n",
       "      <td>424.293032</td>\n",
       "      <td>3.268802</td>\n",
       "      <td>235.714194</td>\n",
       "      <td>34.732101</td>\n",
       "      <td>98.718016</td>\n",
       "      <td>96.154632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A3</td>\n",
       "      <td>200</td>\n",
       "      <td>600</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.04</td>\n",
       "      <td>75.075075</td>\n",
       "      <td>162.919556</td>\n",
       "      <td>64.836000</td>\n",
       "      <td>74.785000</td>\n",
       "      <td>202.89251</td>\n",
       "      <td>...</td>\n",
       "      <td>0.146544</td>\n",
       "      <td>477.581914</td>\n",
       "      <td>1196.764352</td>\n",
       "      <td>0.187700</td>\n",
       "      <td>473.684352</td>\n",
       "      <td>2.587413</td>\n",
       "      <td>1185.893412</td>\n",
       "      <td>11.488857</td>\n",
       "      <td>88.796572</td>\n",
       "      <td>98.164623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A4</td>\n",
       "      <td>250</td>\n",
       "      <td>600</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.04</td>\n",
       "      <td>77.160494</td>\n",
       "      <td>158.751667</td>\n",
       "      <td>66.730000</td>\n",
       "      <td>88.236000</td>\n",
       "      <td>209.86929</td>\n",
       "      <td>...</td>\n",
       "      <td>0.149677</td>\n",
       "      <td>527.316597</td>\n",
       "      <td>1184.030842</td>\n",
       "      <td>0.195563</td>\n",
       "      <td>468.609390</td>\n",
       "      <td>3.127413</td>\n",
       "      <td>1187.789778</td>\n",
       "      <td>15.040303</td>\n",
       "      <td>90.398043</td>\n",
       "      <td>98.596870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A5</td>\n",
       "      <td>300</td>\n",
       "      <td>600</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.04</td>\n",
       "      <td>86.206897</td>\n",
       "      <td>174.770222</td>\n",
       "      <td>106.973000</td>\n",
       "      <td>112.785333</td>\n",
       "      <td>216.84608</td>\n",
       "      <td>...</td>\n",
       "      <td>0.147608</td>\n",
       "      <td>484.400021</td>\n",
       "      <td>1206.942752</td>\n",
       "      <td>0.183212</td>\n",
       "      <td>448.197100</td>\n",
       "      <td>3.350449</td>\n",
       "      <td>701.209390</td>\n",
       "      <td>13.850166</td>\n",
       "      <td>92.783454</td>\n",
       "      <td>98.401670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A6</td>\n",
       "      <td>350</td>\n",
       "      <td>600</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.04</td>\n",
       "      <td>94.086022</td>\n",
       "      <td>192.218556</td>\n",
       "      <td>149.591333</td>\n",
       "      <td>139.725333</td>\n",
       "      <td>288.02816</td>\n",
       "      <td>...</td>\n",
       "      <td>0.133437</td>\n",
       "      <td>492.172767</td>\n",
       "      <td>1198.996442</td>\n",
       "      <td>0.184214</td>\n",
       "      <td>439.896708</td>\n",
       "      <td>3.572343</td>\n",
       "      <td>770.953270</td>\n",
       "      <td>28.667615</td>\n",
       "      <td>92.225593</td>\n",
       "      <td>97.418152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A7</td>\n",
       "      <td>400</td>\n",
       "      <td>600</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.04</td>\n",
       "      <td>100.401606</td>\n",
       "      <td>206.360333</td>\n",
       "      <td>162.941667</td>\n",
       "      <td>116.344667</td>\n",
       "      <td>370.34718</td>\n",
       "      <td>...</td>\n",
       "      <td>0.152210</td>\n",
       "      <td>491.092113</td>\n",
       "      <td>1171.305996</td>\n",
       "      <td>0.171103</td>\n",
       "      <td>443.113115</td>\n",
       "      <td>3.815117</td>\n",
       "      <td>563.939891</td>\n",
       "      <td>33.257468</td>\n",
       "      <td>92.424152</td>\n",
       "      <td>97.975514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A8</td>\n",
       "      <td>450</td>\n",
       "      <td>600</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.04</td>\n",
       "      <td>104.166667</td>\n",
       "      <td>241.679889</td>\n",
       "      <td>214.914000</td>\n",
       "      <td>129.005000</td>\n",
       "      <td>408.00061</td>\n",
       "      <td>...</td>\n",
       "      <td>0.145219</td>\n",
       "      <td>483.527842</td>\n",
       "      <td>1141.179386</td>\n",
       "      <td>0.170161</td>\n",
       "      <td>409.578037</td>\n",
       "      <td>4.122137</td>\n",
       "      <td>255.723029</td>\n",
       "      <td>28.221906</td>\n",
       "      <td>95.441366</td>\n",
       "      <td>97.129787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>A9</td>\n",
       "      <td>500</td>\n",
       "      <td>600</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.04</td>\n",
       "      <td>111.408200</td>\n",
       "      <td>249.463889</td>\n",
       "      <td>157.413000</td>\n",
       "      <td>143.869000</td>\n",
       "      <td>430.47049</td>\n",
       "      <td>...</td>\n",
       "      <td>0.145281</td>\n",
       "      <td>479.539642</td>\n",
       "      <td>1117.452500</td>\n",
       "      <td>0.171681</td>\n",
       "      <td>386.135419</td>\n",
       "      <td>4.274829</td>\n",
       "      <td>79.963833</td>\n",
       "      <td>32.067330</td>\n",
       "      <td>97.603875</td>\n",
       "      <td>96.822279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A11</td>\n",
       "      <td>250</td>\n",
       "      <td>800</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.04</td>\n",
       "      <td>89.798851</td>\n",
       "      <td>132.856111</td>\n",
       "      <td>68.176667</td>\n",
       "      <td>45.220667</td>\n",
       "      <td>186.39335</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144686</td>\n",
       "      <td>444.750062</td>\n",
       "      <td>1155.509820</td>\n",
       "      <td>0.176332</td>\n",
       "      <td>408.848420</td>\n",
       "      <td>2.700466</td>\n",
       "      <td>1159.013639</td>\n",
       "      <td>15.486012</td>\n",
       "      <td>95.242886</td>\n",
       "      <td>98.253717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A12</td>\n",
       "      <td>300</td>\n",
       "      <td>800</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.04</td>\n",
       "      <td>73.818898</td>\n",
       "      <td>150.007556</td>\n",
       "      <td>86.974000</td>\n",
       "      <td>49.185000</td>\n",
       "      <td>200.34692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.169244</td>\n",
       "      <td>507.119295</td>\n",
       "      <td>1131.401270</td>\n",
       "      <td>0.170159</td>\n",
       "      <td>439.707519</td>\n",
       "      <td>3.888416</td>\n",
       "      <td>980.992957</td>\n",
       "      <td>0.023708</td>\n",
       "      <td>92.775969</td>\n",
       "      <td>98.425591</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  No.\\n(New)  Laser power  Scan speed  Line spacing  Layer thickness  \\\n",
       "0         A2          400         400         0.212             0.04   \n",
       "1         A3          200         600         0.111             0.04   \n",
       "2         A4          250         600         0.135             0.04   \n",
       "3         A5          300         600         0.145             0.04   \n",
       "4         A6          350         600         0.155             0.04   \n",
       "5         A7          400         600         0.166             0.04   \n",
       "6         A8          450         600         0.180             0.04   \n",
       "7         A9          500         600         0.187             0.04   \n",
       "8        A11          250         800         0.087             0.04   \n",
       "9        A12          300         800         0.127             0.04   \n",
       "\n",
       "   Energy density      MPST-W      MPST-D      MPST-H     MPAB-W  ...  \\\n",
       "0      117.924528  240.653667  264.694333   63.358667  374.69667  ...   \n",
       "1       75.075075  162.919556   64.836000   74.785000  202.89251  ...   \n",
       "2       77.160494  158.751667   66.730000   88.236000  209.86929  ...   \n",
       "3       86.206897  174.770222  106.973000  112.785333  216.84608  ...   \n",
       "4       94.086022  192.218556  149.591333  139.725333  288.02816  ...   \n",
       "5      100.401606  206.360333  162.941667  116.344667  370.34718  ...   \n",
       "6      104.166667  241.679889  214.914000  129.005000  408.00061  ...   \n",
       "7      111.408200  249.463889  157.413000  143.869000  430.47049  ...   \n",
       "8       89.798851  132.856111   68.176667   45.220667  186.39335  ...   \n",
       "9       73.818898  150.007556   86.974000   49.185000  200.34692  ...   \n",
       "\n",
       "   TP-HD-EB    TP-HD-YS    TP-VD-UTS  TP-VD-EB    TP-VD-YS  Productivity  \\\n",
       "0  0.133932  428.316826  1210.143161  0.161242  424.293032      3.268802   \n",
       "1  0.146544  477.581914  1196.764352  0.187700  473.684352      2.587413   \n",
       "2  0.149677  527.316597  1184.030842  0.195563  468.609390      3.127413   \n",
       "3  0.147608  484.400021  1206.942752  0.183212  448.197100      3.350449   \n",
       "4  0.133437  492.172767  1198.996442  0.184214  439.896708      3.572343   \n",
       "5  0.152210  491.092113  1171.305996  0.171103  443.113115      3.815117   \n",
       "6  0.145219  483.527842  1141.179386  0.170161  409.578037      4.122137   \n",
       "7  0.145281  479.539642  1117.452500  0.171681  386.135419      4.274829   \n",
       "8  0.144686  444.750062  1155.509820  0.176332  408.848420      2.700466   \n",
       "9  0.169244  507.119295  1131.401270  0.170159  439.707519      3.888416   \n",
       "\n",
       "   Deformation  DA-Overmelting  DA-Accuracy  DA-Precision  \n",
       "0   235.714194       34.732101    98.718016     96.154632  \n",
       "1  1185.893412       11.488857    88.796572     98.164623  \n",
       "2  1187.789778       15.040303    90.398043     98.596870  \n",
       "3   701.209390       13.850166    92.783454     98.401670  \n",
       "4   770.953270       28.667615    92.225593     97.418152  \n",
       "5   563.939891       33.257468    92.424152     97.975514  \n",
       "6   255.723029       28.221906    95.441366     97.129787  \n",
       "7    79.963833       32.067330    97.603875     96.822279  \n",
       "8  1159.013639       15.486012    95.242886     98.253717  \n",
       "9   980.992957        0.023708    92.775969     98.425591  \n",
       "\n",
       "[10 rows x 33 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('./Printability_map_Data_python.xlsx', sheet_name='As-printed')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2b2b573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No.\\n(New)                               48\n",
       "Laser power                              48\n",
       "Scan speed                               48\n",
       "Line spacing                             48\n",
       "Layer thickness                          48\n",
       "Energy density                           48\n",
       "MPST-W                                   48\n",
       "MPST-D                                   48\n",
       "MPST-H                                   48\n",
       "MPAB-W                                   48\n",
       "MPAB-D                                   48\n",
       "MPAB-H                                   48\n",
       "Spattering-2nd                           48\n",
       "Denudation width\\n: Single track (um)    48\n",
       "Unnamed: 14                              48\n",
       "Unnamed: 15                               0\n",
       "DW-ML                                    42\n",
       "SR-AVG                                   48\n",
       "SR-STDEV                                 48\n",
       "Relative density                         48\n",
       "HV-HD                                    48\n",
       "HV-VD                                    48\n",
       "TP-HD-UTS                                48\n",
       "TP-HD-EB                                 48\n",
       "TP-HD-YS                                 48\n",
       "TP-VD-UTS                                48\n",
       "TP-VD-EB                                 48\n",
       "TP-VD-YS                                 48\n",
       "Productivity                             48\n",
       "Deformation                              48\n",
       "DA-Overmelting                           48\n",
       "DA-Accuracy                              48\n",
       "DA-Precision                             48\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b49fe87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Laser power</th>\n",
       "      <th>Scan speed</th>\n",
       "      <th>Line spacing</th>\n",
       "      <th>Layer thickness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>400</td>\n",
       "      <td>400</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200</td>\n",
       "      <td>600</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>250</td>\n",
       "      <td>600</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>300</td>\n",
       "      <td>600</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>350</td>\n",
       "      <td>600</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>400</td>\n",
       "      <td>600</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>450</td>\n",
       "      <td>600</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>500</td>\n",
       "      <td>600</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>250</td>\n",
       "      <td>800</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>300</td>\n",
       "      <td>800</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>350</td>\n",
       "      <td>800</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>400</td>\n",
       "      <td>800</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>450</td>\n",
       "      <td>800</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>300</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>400</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>500</td>\n",
       "      <td>1200</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>450</td>\n",
       "      <td>1400</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>350</td>\n",
       "      <td>1600</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>400</td>\n",
       "      <td>1600</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>450</td>\n",
       "      <td>1600</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>500</td>\n",
       "      <td>1800</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>250</td>\n",
       "      <td>400</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>300</td>\n",
       "      <td>400</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>400</td>\n",
       "      <td>400</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>300</td>\n",
       "      <td>600</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>350</td>\n",
       "      <td>600</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>400</td>\n",
       "      <td>600</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>450</td>\n",
       "      <td>600</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>500</td>\n",
       "      <td>600</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>300</td>\n",
       "      <td>800</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>350</td>\n",
       "      <td>800</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>400</td>\n",
       "      <td>800</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>450</td>\n",
       "      <td>800</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>500</td>\n",
       "      <td>800</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>450</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>500</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>350</td>\n",
       "      <td>1200</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>400</td>\n",
       "      <td>1200</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>450</td>\n",
       "      <td>1200</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>500</td>\n",
       "      <td>1200</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>450</td>\n",
       "      <td>1400</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>300</td>\n",
       "      <td>400</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>350</td>\n",
       "      <td>400</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>400</td>\n",
       "      <td>400</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>400</td>\n",
       "      <td>600</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>450</td>\n",
       "      <td>600</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>500</td>\n",
       "      <td>600</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>450</td>\n",
       "      <td>800</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Laser power  Scan speed  Line spacing  Layer thickness\n",
       "0           400         400         0.212             0.04\n",
       "1           200         600         0.111             0.04\n",
       "2           250         600         0.135             0.04\n",
       "3           300         600         0.145             0.04\n",
       "4           350         600         0.155             0.04\n",
       "5           400         600         0.166             0.04\n",
       "6           450         600         0.180             0.04\n",
       "7           500         600         0.187             0.04\n",
       "8           250         800         0.087             0.04\n",
       "9           300         800         0.127             0.04\n",
       "10          350         800         0.123             0.04\n",
       "11          400         800         0.138             0.04\n",
       "12          450         800         0.153             0.04\n",
       "13          300        1000         0.100             0.04\n",
       "14          400        1000         0.118             0.04\n",
       "15          500        1200         0.124             0.04\n",
       "16          450        1400         0.085             0.04\n",
       "17          350        1600         0.089             0.04\n",
       "18          400        1600         0.093             0.04\n",
       "19          450        1600         0.099             0.04\n",
       "20          500        1800         0.110             0.04\n",
       "21          250         400         0.152             0.06\n",
       "22          300         400         0.167             0.06\n",
       "23          400         400         0.137             0.06\n",
       "24          300         600         0.136             0.06\n",
       "25          350         600         0.139             0.06\n",
       "26          400         600         0.087             0.06\n",
       "27          450         600         0.166             0.06\n",
       "28          500         600         0.191             0.06\n",
       "29          300         800         0.112             0.06\n",
       "30          350         800         0.129             0.06\n",
       "31          400         800         0.138             0.06\n",
       "32          450         800         0.120             0.06\n",
       "33          500         800         0.129             0.06\n",
       "34          450        1000         0.098             0.06\n",
       "35          500        1000         0.111             0.06\n",
       "36          350        1200         0.062             0.06\n",
       "37          400        1200         0.073             0.06\n",
       "38          450        1200         0.085             0.06\n",
       "39          500        1200         0.117             0.06\n",
       "40          450        1400         0.097             0.06\n",
       "41          300         400         0.160             0.08\n",
       "42          350         400         0.180             0.08\n",
       "43          400         400         0.153             0.08\n",
       "44          400         600         0.118             0.08\n",
       "45          450         600         0.109             0.08\n",
       "46          500         600         0.123             0.08\n",
       "47          450         800         0.102             0.08"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data = df[['Laser power', 'Scan speed', 'Line spacing','Layer thickness']] \n",
    "x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19ca2267",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"MPST-D\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6af2343c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data_denudation_width = df[[key]].copy();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61fc5fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data.reset_index(drop=True, inplace=True)\n",
    "y_data_denudation_width.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dcce3e19",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "dlopen(/Users/youjeongpark/anaconda3/envs/py37/lib/python3.7/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n  Referenced from: /Users/youjeongpark/anaconda3/envs/py37/lib/python3.7/site-packages/lightgbm/lib/lib_lightgbm.so\n  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/rs/z8t60yls0wn1342s0h5ykkv00000gn/T/ipykernel_39497/4270349355.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGradientBoostingRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mXGBRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mlightgbm\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/lightgbm/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbasic\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBooster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregister_logger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEarlyStopException\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_evaluation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord_evaluation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset_parameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCVBooster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0m_LIB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCDLL\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m     \u001b[0m_LIB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_lib\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m_load_lib\u001b[0;34m()\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;34m\"\"\"Load LightGBM library.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0mlib_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_lib_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m     \u001b[0mlib\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoadLibrary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlib_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m     \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLGBM_GetLastError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_char_p\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0mcallback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCFUNCTYPE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_char_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/ctypes/__init__.py\u001b[0m in \u001b[0;36mLoadLibrary\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mLoadLibrary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dlltype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0mcdll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLibraryLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCDLL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/ctypes/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: dlopen(/Users/youjeongpark/anaconda3/envs/py37/lib/python3.7/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n  Referenced from: /Users/youjeongpark/anaconda3/envs/py37/lib/python3.7/site-packages/lightgbm/lib/lib_lightgbm.so\n  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file)"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f016cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = pd.concat([x_data, y_data_denudation_width], axis=1)\n",
    "df_ = df_.dropna()\n",
    "df_ = df_.copy();\n",
    "df_y = df_[[key]]\n",
    "df_x = df_.drop([key] , axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9878208",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.2, random_state=0)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5612541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Laser power        38\n",
      "Scan speed         38\n",
      "Line spacing       38\n",
      "Layer thickness    38\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(X_train.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "676f1c5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lgb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/rs/z8t60yls0wn1342s0h5ykkv00000gn/T/ipykernel_39497/302961496.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlgbm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLGBMRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlgbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lgb' is not defined"
     ]
    }
   ],
   "source": [
    "lgbm = lgb.LGBMRegressor()\n",
    "lgbm.fit(X_train, y_train.values.ravel())\n",
    "y_pred = lgbm.predict(X_test)\n",
    "\n",
    "\n",
    "# print out the prediction scores\n",
    "print('RMSE: {}'.format(np.sqrt(mean_squared_error(y_test, y_pred))))\n",
    "print('MAE: {}'.format(mean_absolute_error(y_test, y_pred)))\n",
    "print('R-squared: {}'.format(r2_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "057fd188",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/rs/z8t60yls0wn1342s0h5ykkv00000gn/T/ipykernel_39497/372003316.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'orange'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinestyle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'--'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'prediction'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'true values'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "plt.scatter(y_pred, y_test)\n",
    "plt.plot(np.linspace(0,1000,1000), np.linspace(0,1000,1000), c = 'orange', linestyle='--')\n",
    "plt.xlabel('prediction')\n",
    "plt.ylabel('true values')\n",
    "plt.xlim(0,1000)\n",
    "plt.ylim(0,1000)\n",
    "plt.title('Predicted vs True values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e88ab1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the boxplot of area distribution\n",
    "plt.boxplot(df_y, vert=False)\n",
    "plt.title('Area Distribution')\n",
    "plt.xlabel('area')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3130cebf",
   "metadata": {},
   "source": [
    "# Implement SmoteR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88786f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9387cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevance(x):\n",
    "    x = np.array(x)\n",
    "    return sigmoid(x - 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57e2955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot relevance function\n",
    "plt.plot(np.linspace(30, 70, 1000), relevance(np.linspace(30, 70, 1000)))\n",
    "plt.title('Relevance Function')\n",
    "plt.xlabel('x')\n",
    "plt.axhline(y=0, c='gray', linestyle='--')\n",
    "plt.axvline(x=50, c='gray', linestyle='--')\n",
    "plt.ylabel('relevance(x)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da6bd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synth_cases(D, target, o=200, k=3, categorical_col = []):\n",
    "    '''\n",
    "    Function to generate the new cases.\n",
    "    INPUT:\n",
    "        D - pd.DataFrame with the initial data\n",
    "        target - string name of the target column in the dataset\n",
    "        o - oversampling rate\n",
    "        k - number of nearest neighbors to use for the generation\n",
    "        categorical_col - list of categorical column names\n",
    "    OUTPUT:\n",
    "        new_cases - pd.DataFrame containing new generated cases\n",
    "    '''\n",
    "    new_cases = pd.DataFrame(columns = D.columns) # initialize the list of new cases \n",
    "    ng = o // 100 # the number of new cases to generate\n",
    "    for index, case in D.iterrows():\n",
    "        # find k nearest neighbors of the case\n",
    "        knn = KNeighborsRegressor(n_neighbors = k+1) # k+1 because the case is the nearest neighbor to itself\n",
    "        knn.fit(D.drop(columns = [target]).values, D[[target]])\n",
    "        neighbors = knn.kneighbors(case.drop(labels = [target]).values.reshape(1, -1), return_distance=False).reshape(-1)\n",
    "        neighbors = np.delete(neighbors, np.where(neighbors == index))\n",
    "        for i in range(0, ng):\n",
    "            # randomly choose one of the neighbors\n",
    "            x = D.iloc[neighbors[np.random.randint(k)]]\n",
    "            attr = {}          \n",
    "            for a in D.columns:\n",
    "                # skip target column\n",
    "                if a == target:\n",
    "                    continue;\n",
    "                if a in categorical_col:\n",
    "                    # if categorical then choose randomly one of values\n",
    "                    if np.random.randint(2) == 0:\n",
    "                        attr[a] = case[a]\n",
    "                    else:\n",
    "                        attr[a] = x[a]\n",
    "                else:\n",
    "                    # if continious column\n",
    "                    diff = case[a] - x[a]\n",
    "                    attr[a] = case[a] + np.random.randint(2) * diff\n",
    "            # decide the target column\n",
    "            new = np.array(list(attr.values()))\n",
    "            d1 = cosine_similarity(new.reshape(1, -1), case.drop(labels = [target]).values.reshape(1, -1))[0][0]\n",
    "            d2 = cosine_similarity(new.reshape(1, -1), x.drop(labels = [target]).values.reshape(1, -1))[0][0]\n",
    "            attr[target] = (d2 * case[target] + d1 * x[target]) / (d1 + d2)\n",
    "            \n",
    "            # append the result\n",
    "            new_cases = new_cases.append(attr,ignore_index = True)\n",
    "                    \n",
    "    return new_cases\n",
    "\n",
    "def SmoteR(D, target, th = 0.999, o = 200, u = 100, k = 3, categorical_col = []):\n",
    "    '''\n",
    "    The implementation of SmoteR algorithm:\n",
    "    https://core.ac.uk/download/pdf/29202178.pdf\n",
    "    INPUT:\n",
    "        D - pd.DataFrame - the initial dataset\n",
    "        target - the name of the target column in the dataset\n",
    "        th - relevance threshold\n",
    "        o - oversampling rate\n",
    "        u - undersampling rate\n",
    "        k - the number of nearest neighbors\n",
    "    OUTPUT:\n",
    "        new_D - the resulting new dataset\n",
    "    '''\n",
    "    # median of the target variable\n",
    "    y_bar = D[target].median()\n",
    "    \n",
    "    # find rare cases where target less than median\n",
    "    rareL = D[(relevance(D[target]) > th) & (D[target] > y_bar)]  \n",
    "    # generate rare cases for rareL\n",
    "    new_casesL = get_synth_cases(rareL, target, o, k , categorical_col)\n",
    "    \n",
    "    # find rare cases where target greater than median\n",
    "    rareH = D[(relevance(D[target]) > th) & (D[target] < y_bar)]\n",
    "    # generate rare cases for rareH\n",
    "    new_casesH = get_synth_cases(rareH, target, o, k , categorical_col)\n",
    "    \n",
    "    new_cases = pd.concat([new_casesL, new_casesH], axis=0)\n",
    "    \n",
    "    # undersample norm cases\n",
    "    norm_cases = D[relevance(D[target]) <= th]\n",
    "    # get the number of norm cases\n",
    "    nr_norm = int(len(norm_cases) * u / 100)\n",
    "    \n",
    "    norm_cases = norm_cases.sample(min(len(D[relevance(D[target]) <= th]), nr_norm))\n",
    "    \n",
    "    # get the resulting dataset\n",
    "    new_D = pd.concat([new_cases, norm_cases], axis=0)\n",
    "    \n",
    "    return new_D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f92d38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = X_train.columns.tolist()\n",
    "cols.append(key)\n",
    "cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3deef9",
   "metadata": {},
   "source": [
    "# SmoteR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2462686",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_Origin = pd.DataFrame(np.concatenate([X_train, y_train], axis=1), columns = cols)\n",
    "D_Origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a1dd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(43)\n",
    "X_SmoteR = SmoteR(D_Origin, target=key, th = 0.999, o = 180, u = 100, k = 5) # SMmoteR \n",
    "#Xs = Xs[Xs['Denudation width'] != 0]\n",
    "X_SmoteR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661ad21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_Origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbff5d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_smoteR = pd.concat([X_SmoteR, D_Origin])\n",
    "cleaned_smoteR = concat_smoteR.drop_duplicates()\n",
    "cleaned_smoteR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec2f785",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_smoteR.to_csv(\"SmoteR.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91de8e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = cleaned_smoteR.drop(columns=[key])\n",
    "y_train = cleaned_smoteR[[key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676b668f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X train ê°¯ìˆ˜ : \" , X_train.count());\n",
    "print(\"y train ê°¯ìˆ˜ : \" , y_train.count());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd30aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm = lgb.LGBMRegressor()\n",
    "lgbm.fit(X_train, y_train.values.ravel())\n",
    "y_pred = lgbm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2008cbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the prediction scores\n",
    "print('RMSE: {}'.format(np.sqrt(mean_squared_error(y_test, y_pred))))\n",
    "print('MAE: {}'.format(mean_absolute_error(y_test, y_pred)))\n",
    "print('R-squared: {}'.format(r2_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f975e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_pred, y_test)\n",
    "plt.plot(np.linspace(0,1000,1000), np.linspace(0,1000,1000), c = 'orange', linestyle='--')\n",
    "plt.xlabel('prediction')\n",
    "plt.ylabel('true values')\n",
    "plt.xlim(0,1000)\n",
    "plt.ylim(0,1000)\n",
    "plt.title('Predicted vs True values')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcf30d2",
   "metadata": {},
   "source": [
    "# Add SMOGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a0f163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bisect as bs\n",
    "import numpy.random as rd\n",
    "\n",
    "## calculate parameters for phi relevance function\n",
    "def phi_ctrl_pts(\n",
    "    \n",
    "    ## arguments / inputs\n",
    "    y,                    ## response variable y\n",
    "    method = \"auto\",      ## relevance method (\"auto\" or \"manual\")\n",
    "    xtrm_type = \"both\",   ## distribution focus (\"high\", \"low\", \"both\")\n",
    "    coef = 1.5,           ## coefficient for box plot\n",
    "    ctrl_pts = None       ## input for \"manual\" rel method\n",
    "    \n",
    "    ):\n",
    "    \n",
    "    \"\"\" \n",
    "    generates the parameters required for the 'phi()' function, specifies the \n",
    "    regions of interest or 'relevance' in the response variable y, the notion \n",
    "    of relevance can be associated with rarity\n",
    "    \n",
    "    controls how the relevance parameters are calculated by selecting between \n",
    "    two methods, either \"auto\" or \"manual\"\n",
    "    \n",
    "    the \"auto\" method calls the function 'phi_extremes()' and calculates the \n",
    "    relevance parameters by the values beyond the interquartile range\n",
    "    \n",
    "    the \"manual\" method calls the function 'phi_range()' and determines the \n",
    "    relevance parameters by user specification (the use of a domain expert \n",
    "    is recommended for utilizing this method)\n",
    "    \n",
    "    returns a dictionary containing 3 items \"method\", \"num_pts\", \"ctrl_pts\": \n",
    "    1) the \"method\" item contains a chartacter string simply indicating the \n",
    "    method used calculate the relevance parameters (control points) either \n",
    "    \"auto\" or \"manual\"\n",
    "    \n",
    "    2) the \"num_pts\" item contains a positive integer simply indicating the \n",
    "    number of relevance parameters returned, typically 3\n",
    "    \n",
    "    3) the \"ctrl_pts\" item contains an array indicating the regions of \n",
    "    interest in the response variable y and their corresponding relevance \n",
    "    values mapped to either 0 or 1, expressed as [y, 0, 1]\n",
    "    \n",
    "    ref:\n",
    "    \n",
    "    Branco, P., Ribeiro, R., Torgo, L. (2017).\n",
    "    Package 'UBL'. The Comprehensive R Archive Network (CRAN).\n",
    "    https://cran.r-project.org/web/packages/UBL/UBL.pdf.\n",
    "    \n",
    "    Ribeiro, R. (2011). Utility-Based Regression.\n",
    "    (PhD Dissertation, Dept. Computer Science, \n",
    "    Faculty of Sciences, University of Porto).\n",
    "    \"\"\"\n",
    "    \n",
    "    ## quality control check for response variable 'y'\n",
    "    if any(y == None) or isinstance(y, (int, float, complex)):\n",
    "        print(\"response variable 'y' must be specified and numeric\")\n",
    "    \n",
    "    ## quality control check for user specified method\n",
    "    if method in [\"auto\", \"manual\"] == False:\n",
    "        print(\"method must be either: 'auto' or 'manual'\")\n",
    "    \n",
    "    ## conduct 'extremes' method (default)\n",
    "    if method == \"auto\":\n",
    "        phi_params = phi_extremes(y, xtrm_type, coef, ctrl_pts)\n",
    "    \n",
    "    ## conduct 'range' method\n",
    "    if method == \"manual\":\n",
    "        phi_params = phi_range(y, xtrm_type, coef, ctrl_pts)\n",
    "    \n",
    "    ## return phi relevance parameters dictionary\n",
    "    return phi_params\n",
    "\n",
    "## calculates phi parameters for statistically extreme values\n",
    "def phi_extremes(y, xtrm_type, coef, ctrl_pts):\n",
    "  \n",
    "    \"\"\" \n",
    "    assigns relevance to the most extreme values in the distribution of response \n",
    "    variable y according to the box plot stats generated from 'box_plot_stat()'\n",
    "    \"\"\"\n",
    "    \n",
    "    ## create 'ctrl_pts' variable\n",
    "    ctrl_pts = []\n",
    "    \n",
    "    ## calculates statistically extreme values by\n",
    "    ## box plot stats in the response variable y\n",
    "    ## (see function 'boxplot_stats()' for details)\n",
    "    bx_plt_st = box_plot_stats(y, coef)\n",
    "    \n",
    "    ## calculate range of the response variable y\n",
    "    rng = [y.min(), y.max()]\n",
    "    \n",
    "    ## adjust low\n",
    "    if xtrm_type in [\"both\", \"low\"] and any(bx_plt_st[\"xtrms\"]\n",
    "    < bx_plt_st[\"stats\"][0]):\n",
    "        ctrl_pts.extend([bx_plt_st[\"stats\"][0], 1, 0])\n",
    "   \n",
    "    ## min\n",
    "    else:\n",
    "        ctrl_pts.extend([rng[0], 0, 0])\n",
    "      \n",
    "    ## median\n",
    "    if bx_plt_st[\"stats\"][2] != rng[0]:\n",
    "        ctrl_pts.extend([bx_plt_st[\"stats\"][2], 0, 0])\n",
    "    \n",
    "    ## adjust high\n",
    "    if xtrm_type in [\"both\", \"high\"] and any(bx_plt_st[\"xtrms\"]\n",
    "    > bx_plt_st[\"stats\"][4]):\n",
    "        ctrl_pts.extend([bx_plt_st[\"stats\"][4], 1, 0])\n",
    "    \n",
    "    ## max\n",
    "    else:\n",
    "        if bx_plt_st[\"stats\"][2] != rng[1]:\n",
    "            ctrl_pts.extend([rng[1], 0, 0])\n",
    "    \n",
    "    ## store phi relevance parameter dictionary\n",
    "    phi_params = {}\n",
    "    phi_params[\"method\"] = \"extremes\"\n",
    "    phi_params[\"num_pts\"] = round(len(ctrl_pts) / 3)\n",
    "    phi_params[\"ctrl_pts\"] = ctrl_pts\n",
    "    \n",
    "    ## return dictionary\n",
    "    return phi_params\n",
    "\n",
    "## calculates phi parameters for user specified range\n",
    "def phi_range(y, xtrm_type, coef, ctrl_pts):\n",
    "    \n",
    "    \"\"\"\n",
    "    assigns relevance to values in the response variable y according to user \n",
    "    specification, when specifying relevant regions use matrix format [x, y, m]\n",
    "    \n",
    "    x is an array of relevant values in the response variable y, y is an array \n",
    "    of values mapped to 1 or 0, and m is typically an array of zeros\n",
    "    \n",
    "    m is the phi derivative adjusted afterward by the phi relevance function to \n",
    "    interpolate a smooth and continous monotonically increasing function\n",
    "    \n",
    "    example:\n",
    "    [x,  y, m],\n",
    "    [15, 1, 0],\n",
    "    [30, 0, 0],\n",
    "    [55, 1, 0]\n",
    "    \"\"\"\n",
    "    \n",
    "    ## load dependencies\n",
    "    # import numpy as np\n",
    "    \n",
    "    ## convert 'ctrl_pts' to numpy 2d array (matrix)\n",
    "    ctrl_pts = np.array(ctrl_pts)\n",
    "                   \n",
    "    ## quality control checks for user specified phi relevance values\n",
    "    if np.isnan(ctrl_pts).any() or np.size(ctrl_pts, axis = 1) > 3 or np.size(\n",
    "        ctrl_pts, axis = 1) < 2 or not isinstance(ctrl_pts, (np.ndarray)):\n",
    "        print(\"ctrl_pts must be given as a matrix in the form: [x, y, m]\" \n",
    "              \"or [x, y]\")\n",
    "    \n",
    "    if (ctrl_pts[1: ,[1, ]] > 1).any() or (ctrl_pts[1: ,[1, ]] < 0).any():\n",
    "        print(\"phi relevance function only maps values: [0, 1]\")\n",
    "          \n",
    "    ## store number of control points \n",
    "    else:\n",
    "        num_pts = np.size(ctrl_pts, axis = 0)\n",
    "        dx = ctrl_pts[1:,[0,]] - ctrl_pts[0:-1,[0,]]\n",
    "    \n",
    "    ## quality control check for dx\n",
    "    if np.isnan(dx).any() or dx.any() == 0:\n",
    "        print(\"x must strictly increase (not na)\")\n",
    "    \n",
    "    ## sort control points from lowest to highest\n",
    "    else:\n",
    "        ctrl_pts = ctrl_pts[np.argsort(ctrl_pts[:,0])]\n",
    "    \n",
    "    ## calculate for two column user specified control points [x, y]\n",
    "    if np.size(ctrl_pts, axis = 1) == 2:\n",
    "        \n",
    "        ## monotone hermite spline method by fritsch & carlson (monoH.FC)\n",
    "        dx = ctrl_pts[1:,[0,]] - ctrl_pts[0:-1,[0,]]\n",
    "        dy = ctrl_pts[1:,[1,]] - ctrl_pts[0:-1,[1,]]\n",
    "        sx = dy / dx\n",
    "        \n",
    "        ## calculate constant extrapolation\n",
    "        m = np.divide(sx[1:] + sx[0:-1], 2)\n",
    "        m = np.array(ex).ravel().tolist()\n",
    "        m.insert(0, 0)\n",
    "        m.insert(len(ex), 0)\n",
    "        \n",
    "        ## add calculated column 'm' to user specified control points \n",
    "        ## from [x, y] to [x, y, m] and store in 'ctrl_pts'\n",
    "        ctrl_pts = np.insert(ctrl_pts, 2, m, axis = 1)\n",
    "    \n",
    "    ## store phi relevance parameter dictionary\n",
    "    phi_params = {}\n",
    "    phi_params[\"method\"] = \"range\"\n",
    "    phi_params[\"num_pts\"] = np.size(ctrl_pts, axis = 0)\n",
    "    phi_params[\"ctrl_pts\"] = np.array(ctrl_pts).ravel().tolist()\n",
    "    \n",
    "    ## return dictionary\n",
    "    return phi_params\n",
    "\n",
    "## calculate parameters for phi relevance function\n",
    "def phi_ctrl_pts(\n",
    "    \n",
    "    ## arguments / inputs\n",
    "    y,                    ## response variable y\n",
    "    method = \"auto\",      ## relevance method (\"auto\" or \"manual\")\n",
    "    xtrm_type = \"both\",   ## distribution focus (\"high\", \"low\", \"both\")\n",
    "    coef = 1.5,           ## coefficient for box plot\n",
    "    ctrl_pts = None       ## input for \"manual\" rel method\n",
    "    \n",
    "    ):\n",
    "    \n",
    "    \"\"\" \n",
    "    generates the parameters required for the 'phi()' function, specifies the \n",
    "    regions of interest or 'relevance' in the response variable y, the notion \n",
    "    of relevance can be associated with rarity\n",
    "    \n",
    "    controls how the relevance parameters are calculated by selecting between \n",
    "    two methods, either \"auto\" or \"manual\"\n",
    "    \n",
    "    the \"auto\" method calls the function 'phi_extremes()' and calculates the \n",
    "    relevance parameters by the values beyond the interquartile range\n",
    "    \n",
    "    the \"manual\" method calls the function 'phi_range()' and determines the \n",
    "    relevance parameters by user specification (the use of a domain expert \n",
    "    is recommended for utilizing this method)\n",
    "    \n",
    "    returns a dictionary containing 3 items \"method\", \"num_pts\", \"ctrl_pts\": \n",
    "    1) the \"method\" item contains a chartacter string simply indicating the \n",
    "    method used calculate the relevance parameters (control points) either \n",
    "    \"auto\" or \"manual\"\n",
    "    \n",
    "    2) the \"num_pts\" item contains a positive integer simply indicating the \n",
    "    number of relevance parameters returned, typically 3\n",
    "    \n",
    "    3) the \"ctrl_pts\" item contains an array indicating the regions of \n",
    "    interest in the response variable y and their corresponding relevance \n",
    "    values mapped to either 0 or 1, expressed as [y, 0, 1]\n",
    "    \n",
    "    ref:\n",
    "    \n",
    "    Branco, P., Ribeiro, R., Torgo, L. (2017).\n",
    "    Package 'UBL'. The Comprehensive R Archive Network (CRAN).\n",
    "    https://cran.r-project.org/web/packages/UBL/UBL.pdf.\n",
    "    \n",
    "    Ribeiro, R. (2011). Utility-Based Regression.\n",
    "    (PhD Dissertation, Dept. Computer Science, \n",
    "    Faculty of Sciences, University of Porto).\n",
    "    \"\"\"\n",
    "    \n",
    "    ## quality control check for response variable 'y'\n",
    "    if any(y == None) or isinstance(y, (int, float, complex)):\n",
    "        print(\"response variable 'y' must be specified and numeric\")\n",
    "    \n",
    "    ## quality control check for user specified method\n",
    "    if method in [\"auto\", \"manual\"] == False:\n",
    "        print(\"method must be either: 'auto' or 'manual'\")\n",
    "    \n",
    "    ## conduct 'extremes' method (default)\n",
    "    if method == \"auto\":\n",
    "        phi_params = phi_extremes(y, xtrm_type, coef, ctrl_pts)\n",
    "    \n",
    "    ## conduct 'range' method\n",
    "    if method == \"manual\":\n",
    "        phi_params = phi_range(y, xtrm_type, coef, ctrl_pts)\n",
    "    \n",
    "    ## return phi relevance parameters dictionary\n",
    "    return phi_params\n",
    "\n",
    "## calculates phi parameters for statistically extreme values\n",
    "def phi_extremes(y, xtrm_type, coef, ctrl_pts):\n",
    "  \n",
    "    \"\"\" \n",
    "    assigns relevance to the most extreme values in the distribution of response \n",
    "    variable y according to the box plot stats generated from 'box_plot_stat()'\n",
    "    \"\"\"\n",
    "    \n",
    "    ## create 'ctrl_pts' variable\n",
    "    ctrl_pts = []\n",
    "    \n",
    "    ## calculates statistically extreme values by\n",
    "    ## box plot stats in the response variable y\n",
    "    ## (see function 'boxplot_stats()' for details)\n",
    "    bx_plt_st = box_plot_stats(y, coef)\n",
    "    \n",
    "    ## calculate range of the response variable y\n",
    "    rng = [y.min(), y.max()]\n",
    "    \n",
    "    ## adjust low\n",
    "    if xtrm_type in [\"both\", \"low\"] and any(bx_plt_st[\"xtrms\"]\n",
    "    < bx_plt_st[\"stats\"][0]):\n",
    "        ctrl_pts.extend([bx_plt_st[\"stats\"][0], 1, 0])\n",
    "   \n",
    "    ## min\n",
    "    else:\n",
    "        ctrl_pts.extend([rng[0], 0, 0])\n",
    "      \n",
    "    ## median\n",
    "    if bx_plt_st[\"stats\"][2] != rng[0]:\n",
    "        ctrl_pts.extend([bx_plt_st[\"stats\"][2], 0, 0])\n",
    "    \n",
    "    ## adjust high\n",
    "    if xtrm_type in [\"both\", \"high\"] and any(bx_plt_st[\"xtrms\"]\n",
    "    > bx_plt_st[\"stats\"][4]):\n",
    "        ctrl_pts.extend([bx_plt_st[\"stats\"][4], 1, 0])\n",
    "    \n",
    "    ## max\n",
    "    else:\n",
    "        if bx_plt_st[\"stats\"][2] != rng[1]:\n",
    "            ctrl_pts.extend([rng[1], 0, 0])\n",
    "    \n",
    "    ## store phi relevance parameter dictionary\n",
    "    phi_params = {}\n",
    "    phi_params[\"method\"] = \"extremes\"\n",
    "    phi_params[\"num_pts\"] = round(len(ctrl_pts) / 3)\n",
    "    phi_params[\"ctrl_pts\"] = ctrl_pts\n",
    "    \n",
    "    ## return dictionary\n",
    "    return phi_params\n",
    "\n",
    "## calculates phi parameters for user specified range\n",
    "def phi_range(y, xtrm_type, coef, ctrl_pts):\n",
    "    \n",
    "    \"\"\"\n",
    "    assigns relevance to values in the response variable y according to user \n",
    "    specification, when specifying relevant regions use matrix format [x, y, m]\n",
    "    \n",
    "    x is an array of relevant values in the response variable y, y is an array \n",
    "    of values mapped to 1 or 0, and m is typically an array of zeros\n",
    "    \n",
    "    m is the phi derivative adjusted afterward by the phi relevance function to \n",
    "    interpolate a smooth and continous monotonically increasing function\n",
    "    \n",
    "    example:\n",
    "    [x,  y, m],\n",
    "    [15, 1, 0],\n",
    "    [30, 0, 0],\n",
    "    [55, 1, 0]\n",
    "    \"\"\"\n",
    "    \n",
    "    ## load dependencies\n",
    "    # import numpy as np\n",
    "    \n",
    "    ## convert 'ctrl_pts' to numpy 2d array (matrix)\n",
    "    ctrl_pts = np.array(ctrl_pts)\n",
    "                   \n",
    "    ## quality control checks for user specified phi relevance values\n",
    "    if np.isnan(ctrl_pts).any() or np.size(ctrl_pts, axis = 1) > 3 or np.size(\n",
    "        ctrl_pts, axis = 1) < 2 or not isinstance(ctrl_pts, (np.ndarray)):\n",
    "        print(\"ctrl_pts must be given as a matrix in the form: [x, y, m]\" \n",
    "              \"or [x, y]\")\n",
    "    \n",
    "    if (ctrl_pts[1: ,[1, ]] > 1).any() or (ctrl_pts[1: ,[1, ]] < 0).any():\n",
    "        print(\"phi relevance function only maps values: [0, 1]\")\n",
    "          \n",
    "    ## store number of control points \n",
    "    else:\n",
    "        num_pts = np.size(ctrl_pts, axis = 0)\n",
    "        dx = ctrl_pts[1:,[0,]] - ctrl_pts[0:-1,[0,]]\n",
    "    \n",
    "    ## quality control check for dx\n",
    "    if np.isnan(dx).any() or dx.any() == 0:\n",
    "        print(\"x must strictly increase (not na)\")\n",
    "    \n",
    "    ## sort control points from lowest to highest\n",
    "    else:\n",
    "        ctrl_pts = ctrl_pts[np.argsort(ctrl_pts[:,0])]\n",
    "    \n",
    "    ## calculate for two column user specified control points [x, y]\n",
    "    if np.size(ctrl_pts, axis = 1) == 2:\n",
    "        \n",
    "        ## monotone hermite spline method by fritsch & carlson (monoH.FC)\n",
    "        dx = ctrl_pts[1:,[0,]] - ctrl_pts[0:-1,[0,]]\n",
    "        dy = ctrl_pts[1:,[1,]] - ctrl_pts[0:-1,[1,]]\n",
    "        sx = dy / dx\n",
    "        \n",
    "        ## calculate constant extrapolation\n",
    "        m = np.divide(sx[1:] + sx[0:-1], 2)\n",
    "        m = np.array(ex).ravel().tolist()\n",
    "        m.insert(0, 0)\n",
    "        m.insert(len(ex), 0)\n",
    "        \n",
    "        ## add calculated column 'm' to user specified control points \n",
    "        ## from [x, y] to [x, y, m] and store in 'ctrl_pts'\n",
    "        ctrl_pts = np.insert(ctrl_pts, 2, m, axis = 1)\n",
    "    \n",
    "    ## store phi relevance parameter dictionary\n",
    "    phi_params = {}\n",
    "    phi_params[\"method\"] = \"range\"\n",
    "    phi_params[\"num_pts\"] = np.size(ctrl_pts, axis = 0)\n",
    "    phi_params[\"ctrl_pts\"] = np.array(ctrl_pts).ravel().tolist()\n",
    "    \n",
    "    ## return dictionary\n",
    "    return phi_params\n",
    "\n",
    "## calculate box plot statistics\n",
    "def box_plot_stats(\n",
    "    \n",
    "    ## arguments / inputs\n",
    "    x,          ## input array of values \n",
    "    coef = 1.5  ## positive real number\n",
    "                ## (determines how far the whiskers extend from the iqr)\n",
    "    ):          \n",
    "    \n",
    "    \"\"\" \n",
    "    calculates box plot five-number summary: the lower whisker extreme, the \n",
    "    lower â€˜hingeâ€™ (observed value), the median, the upper â€˜hingeâ€™, and upper \n",
    "    whisker extreme (observed value)\n",
    "    \n",
    "    returns a results dictionary containing 2 items: \"stats\" and \"xtrms\"\n",
    "    1) the \"stats\" item contains the box plot five-number summary as an array\n",
    "    2) the \"xtrms\" item contains values which lie beyond the box plot extremes\n",
    "    \n",
    "    functions much the same as R's 'boxplot.stats()' function for which this\n",
    "    Python implementation was predicated\n",
    "    \n",
    "    ref:\n",
    "    \n",
    "    The R Project for Statistical Computing. (2019). Box Plot Statistics. \n",
    "    http://finzi.psych.upenn.edu/R/library/grDevices/html/boxplot.stats.html.\n",
    "    \n",
    "    Tukey, J. W. (1977). Exploratory Data Analysis. Section 2C.\n",
    "    McGill, R., Tukey, J.W. and Larsen, W.A. (1978). Variations of Box Plots. \n",
    "    The American Statistician, 32:12-16. http://dx.doi.org/10.2307/2683468.\n",
    "    Velleman, P.F. and Hoaglin, D.C. (1981). Applications, Basics and \n",
    "    Computing of Exploratory Data Analysis. Duxbury Press.\n",
    "    Emerson, J.D. and Strenio, J. (1983). Boxplots and Batch Comparison. \n",
    "    Chapter 3 of Understanding Robust and Exploratory Data Analysis, \n",
    "    eds. D.C. Hoaglin, F. Mosteller and J.W. Tukey. Wiley.\n",
    "    Chambers, J.M., Cleveland, W.S., Kleiner, B. and Tukey, P.A. (1983). \n",
    "    Graphical Methods for Data Analysis. Wadsworth & Brooks/Cole.\n",
    "    \"\"\"\n",
    "    \n",
    "    ## load dependency\n",
    "    # import numpy as np\n",
    "    \n",
    "    ## convert input to numpy array\n",
    "    x = np.array(x)\n",
    "    \n",
    "    ## determine median, lower â€˜hingeâ€™, upper â€˜hingeâ€™\n",
    "    median = np.quantile(a = x, q = 0.50, interpolation = \"midpoint\")\n",
    "    first_quart = np.quantile(a = x, q = 0.25, interpolation = \"midpoint\")\n",
    "    third_quart = np.quantile(a = x, q = 0.75, interpolation = \"midpoint\")\n",
    "    \n",
    "    ## calculate inter quartile range\n",
    "    intr_quart_rng = third_quart - first_quart\n",
    "    \n",
    "    ## calculate extreme of the lower whisker (observed, not interpolated)\n",
    "    lower = first_quart - (coef * intr_quart_rng)\n",
    "    lower_whisk = np.compress(x >= lower, x)\n",
    "    lower_whisk_obs = np.min(lower_whisk)\n",
    "    \n",
    "    ## calculate extreme of the upper whisker (observed, not interpolated)\n",
    "    upper = third_quart + (coef * intr_quart_rng)\n",
    "    upper_whisk = np.compress(x <= upper, x)\n",
    "    upper_whisk_obs = np.max(upper_whisk)\n",
    "    \n",
    "    ## store box plot results dictionary\n",
    "    boxplot_stats = {}\n",
    "    boxplot_stats[\"stats\"] = np.array([lower_whisk_obs, \n",
    "                                       first_quart, \n",
    "                                       median, \n",
    "                                       third_quart, \n",
    "                                       upper_whisk_obs])\n",
    "   \n",
    "    ## store observations beyond the box plot extremes\n",
    "    boxplot_stats[\"xtrms\"] = np.array(x[(x < lower_whisk_obs) | \n",
    "                                        (x > upper_whisk_obs)])\n",
    "    \n",
    "    ## return dictionary        \n",
    "    return boxplot_stats\n",
    "\n",
    "## calculate the phi relevance function\n",
    "def phi(\n",
    "    \n",
    "    ## arguments / inputs\n",
    "    y,        ## reponse variable y\n",
    "    ctrl_pts  ## params from the 'ctrl_pts()' function\n",
    "    \n",
    "    ):\n",
    "    \n",
    "    \"\"\"\n",
    "    generates a monotonic piecewise cubic spline from a sorted list (ascending)\n",
    "    of the response variable y in order to determine which observations exceed \n",
    "    a given threshold ('rel_thres' argument in the main 'smogn()' function)\n",
    "    \n",
    "    returns an array of length n (number of observations in the training set) of \n",
    "    the phi relevance values corresponding to each observation in y to determine\n",
    "    whether or not an given observation in y is considered 'normal' or 'rare'\n",
    "    \n",
    "    the 'normal' observations get placed into a majority class subset or 'bin' \n",
    "    (normal bin) and are under-sampled, while the 'rare' observations get placed \n",
    "    into seperate minority class subset (rare bin) where they are over-sampled\n",
    "    \n",
    "    the original implementation was as an R foreign function call to C and later \n",
    "    adapted to Fortran 90, but was implemented here in Python for the purposes\n",
    "    of consistency and maintainability\n",
    "    \n",
    "    ref:\n",
    "    \n",
    "    Branco, P., Ribeiro, R., Torgo, L. (2017). \n",
    "    Package 'UBL'. The Comprehensive R Archive Network (CRAN).\n",
    "    https://cran.r-project.org/web/packages/UBL/UBL.pdf.\n",
    "    \n",
    "    Fritsch, F., Carlson, R. (1980).\n",
    "    Monotone Piecewise Cubic Interpolation.\n",
    "    SIAM Journal on Numerical Analysis, 17(2):238-246.\n",
    "    https://doi.org/10.1137/0717021.\n",
    "    \n",
    "    Ribeiro, R. (2011). Utility-Based Regression.\n",
    "    (PhD Dissertation, Dept. Computer Science, \n",
    "    Faculty of Sciences, University of Porto).\n",
    "    \"\"\"\n",
    "    \n",
    "    ## assign variables\n",
    "    y = y                                ## reponse variable y\n",
    "    n = len(y)                           ## number of points in y\n",
    "    num_pts = ctrl_pts[\"num_pts\"]    ## number of control points\n",
    "    ctrl_pts = ctrl_pts[\"ctrl_pts\"]  ## control points\n",
    "    \n",
    "    ## reindex y\n",
    "    y = y.reset_index(drop = True)\n",
    "    \n",
    "    ## initialize phi relevance function\n",
    "    y_phi = phi_init(y, n, num_pts, ctrl_pts)\n",
    "    \n",
    "    ## return phi values\n",
    "    return y_phi\n",
    "\n",
    "## pre-process control points and calculate phi values\n",
    "def phi_init(y, n, num_pts, ctrl_pts):\n",
    "    \n",
    "    ## construct control point arrays\n",
    "    x = []\n",
    "    y_rel = []\n",
    "    m = []\n",
    "    \n",
    "    for i in range(num_pts):\n",
    "        x.append(ctrl_pts[3 * i])\n",
    "        y_rel.append(ctrl_pts[3 * i + 1])\n",
    "        m.append(ctrl_pts[3 * i + 2])\n",
    "    \n",
    "    ## calculate auxilary coefficients for 'pchip_slope_mono_fc()'\n",
    "    h = []\n",
    "    delta = []\n",
    "    \n",
    "    for i in range(num_pts - 1):\n",
    "        h.append(x[i + 1] - x[i])\n",
    "        delta.append((y_rel[i + 1] - y_rel[i]) / h[i])\n",
    "    \n",
    "    ## conduct monotone piecewise cubic interpolation\n",
    "    m_adj = pchip_slope_mono_fc(m, delta, num_pts)\n",
    "    \n",
    "    ## assign variables for 'pchip_val()'\n",
    "    a = y_rel\n",
    "    b = m_adj\n",
    "    \n",
    "    ## calculate auxilary coefficients for 'pchip_val()'\n",
    "    c = []\n",
    "    d = []\n",
    "    \n",
    "    for i in range(num_pts - 1):\n",
    "        c.append((3 * delta[i] - 2 * m_adj[i] - m_adj[i + 1]) / h[i])\n",
    "        d.append((m_adj[i] - 2 * delta[i] + m_adj[i + 1]) / (h[i] * h[i]))\n",
    "    \n",
    "    ## calculate phi values\n",
    "    y_phi = [None] * n\n",
    "    \n",
    "    for i in range(n):\n",
    "        y_phi[i] = pchip_val(y[i], x, a, b, c, d, num_pts)\n",
    "    \n",
    "    ## return phi values to the higher function 'phi()'\n",
    "    return y_phi\n",
    "\n",
    "## calculate slopes for shape preserving hermite cubic polynomials\n",
    "def pchip_slope_mono_fc(m, delta, num_pts):\n",
    "    \n",
    "    for k in range(num_pts - 1):\n",
    "        sk = delta[k]\n",
    "        k1 = k + 1\n",
    "        \n",
    "        if abs(sk) == 0:\n",
    "            m[k] = m[k1] = 0\n",
    "        \n",
    "        else:\n",
    "            alpha = m[k] / sk\n",
    "            beta = m[k1] / sk\n",
    "            \n",
    "            if abs(m[k]) != 0 and alpha < 0:\n",
    "                m[k] = -m[k]\n",
    "                alpha = m[k] / sk\n",
    "            \n",
    "            if abs(m[k1]) != 0 and beta < 0:\n",
    "                m[k1] = -m[k1]\n",
    "                beta = m[k1] / sk\n",
    "            \n",
    "            ## pre-process for monotoncity check\n",
    "            m_2ab3 = 2 * alpha + beta - 3\n",
    "            m_a2b3 = alpha + 2 * beta - 3\n",
    "            \n",
    "            ## check for monotoncity\n",
    "            if m_2ab3 > 0 and m_a2b3 > 0 and alpha * (\n",
    "                m_2ab3 + m_a2b3) < (m_2ab3 * m_2ab3):\n",
    "                \n",
    "                ## fix slopes if outside of monotoncity\n",
    "                taus = 3 * sk / sqrt(alpha * alpha + beta * beta)\n",
    "                m[k] = taus * alpha\n",
    "                m[k1] = taus * beta\n",
    "    \n",
    "    ## return adjusted slopes m\n",
    "    return m\n",
    "\n",
    "## calculate phi values based on monotone piecewise cubic interpolation\n",
    "def pchip_val(y, x, a, b, c, d, num_pts):\n",
    "    \n",
    "    ## load dependency\n",
    "    # import bisect as bs\n",
    "    \n",
    "    ## find interval that contains or is nearest to y\n",
    "    i = bs.bisect(\n",
    "        \n",
    "        a = x,  ## array of relevance values\n",
    "        x = y   ## single observation in y\n",
    "        ) - 1   ## minus 1 to match index position\n",
    "    \n",
    "    ## calculate phi values\n",
    "    if i == num_pts - 1:\n",
    "        y_val = a[i] + b[i] * (y - x[i])\n",
    "    \n",
    "    elif i < 0:\n",
    "        y_val = 1\n",
    "    \n",
    "    else:\n",
    "        s = y - x[i]\n",
    "        y_val = a[i] + s * (b[i] + s * (c[i] + s * d[i]))\n",
    "    \n",
    "    ## return phi values to the higher function 'phi_init()'\n",
    "    return y_val\n",
    "\n",
    "## generate synthetic observations\n",
    "def synth_gen(\n",
    "    \n",
    "    ## arguments / inputs\n",
    "    data,       ## training set\n",
    "    index,      ## index of input data\n",
    "    perc,       ## % over / under sampling\n",
    "    k           ## num of neighs for over-sampling\n",
    "    \n",
    "    ):\n",
    "    \n",
    "    \"\"\"\n",
    "    generates synthetic observations and is the primary function underlying the\n",
    "    over-sampling technique utilized in the higher main function 'smogn()', the\n",
    "    4 step procedure for generating synthetic observations is:\n",
    "    \n",
    "    1) pre-processing: temporarily removes features without variation, label \n",
    "    encodes nominal / categorical features, and subsets the training set into \n",
    "    two data sets by data type: numeric / continuous, and nominal / categorical\n",
    "    \n",
    "    2) distances: calculates the cartesian distances between all observations, \n",
    "    distance metric automatically determined by data type (euclidean distance \n",
    "    for numeric only data, heom distance for both numeric and nominal data, and \n",
    "    hamming distance for nominal only data) and determine k nearest neighbors\n",
    "    \n",
    "    3) over-sampling: selects between two techniques, either synthetic minority \n",
    "    over-sampling technique for regression 'smoter' or 'smoter-gn' which applies\n",
    "    a similar interpolation method to 'smoter', but perterbs the interpolated \n",
    "    values\n",
    "    \n",
    "    'smoter' is selected when the distance between a given observation and a \n",
    "    selected nearest neighbor is within the maximum threshold (half the median \n",
    "    distance of k nearest neighbors) 'smoter-gn' is selected when a given \n",
    "    observation and a selected nearest neighbor exceeds that same threshold\n",
    "    \n",
    "    both 'smoter' and 'smoter-gn' only applies to numeric / continuous features, \n",
    "    for nominal / categorical features, synthetic values are generated at random \n",
    "    from sampling observed values found within the same feature\n",
    "    \n",
    "    4) post processing: restores original values for label encoded features, \n",
    "    reintroduces constant features previously removed, converts any interpolated\n",
    "    negative values to zero in the case of non-negative features\n",
    "    \n",
    "    returns a pandas dataframe containing synthetic observations of the training\n",
    "    set which are then returned to the higher main function 'smogn()'\n",
    "    \n",
    "    ref:\n",
    "    \n",
    "    Branco, P., Torgo, L., Ribeiro, R. (2017).\n",
    "    SMOGN: A Pre-Processing Approach for Imbalanced Regression.\n",
    "    Proceedings of Machine Learning Research, 74:36-50.\n",
    "    http://proceedings.mlr.press/v74/branco17a/branco17a.pdf.\n",
    "    \n",
    "    Branco, P., Ribeiro, R., Torgo, L. (2017). \n",
    "    Package 'UBL'. The Comprehensive R Archive Network (CRAN).\n",
    "    https://cran.r-project.org/web/packages/UBL/UBL.pdf.\n",
    "    \"\"\"\n",
    "    \n",
    "    ## load dependencies\n",
    "    # import numpy as np\n",
    "    # import pandas as pd\n",
    "    # import random as rd\n",
    "    \n",
    "    ## subset original dataframe by bump classification index\n",
    "    data = data.iloc[index]\n",
    "    \n",
    "    ## store dimensions of data subset\n",
    "    n = len(data)\n",
    "    d = len(data.columns)\n",
    "    \n",
    "    ## store original data types\n",
    "    feat_dtypes_orig = [None] * d\n",
    "    \n",
    "    for j in range(d):\n",
    "        feat_dtypes_orig[j] = data.iloc[:, j].dtype\n",
    "    \n",
    "    ## find non-negative numeric features\n",
    "    feat_non_neg = [] \n",
    "    num_dtypes = [\"int64\", \"float64\"]\n",
    "    \n",
    "    for j in range(d):\n",
    "        if data.iloc[:, j].dtype in num_dtypes and any(data.iloc[:, j] > 0):\n",
    "            feat_non_neg.append(j)\n",
    "    \n",
    "    ## find features without variation (constant features)\n",
    "    feat_const = data.columns[data.nunique() == 1]\n",
    "    \n",
    "    ## temporarily remove constant features\n",
    "    if len(feat_const) > 0:\n",
    "        \n",
    "        ## create copy of orignal data and omit constant features\n",
    "        data_orig = data.copy()\n",
    "        data = data.drop(data.columns[feat_const], axis = 1)\n",
    "        \n",
    "        ## store list of features with variation\n",
    "        feat_var = list(data.columns.values)\n",
    "        \n",
    "        ## reindex features with variation\n",
    "        for i in range(d - len(feat_const)):\n",
    "            data.rename(columns = {\n",
    "                data.columns[i]: i\n",
    "                }, inplace = True)\n",
    "        \n",
    "        ## store new dimension of feature space\n",
    "        d = len(data.columns)\n",
    "    \n",
    "    ## create copy of data containing variation\n",
    "    data_var = data.copy()\n",
    "    \n",
    "    ## create global feature list by column index\n",
    "    feat_list = list(data.columns.values)\n",
    "    \n",
    "    ## create nominal feature list and\n",
    "    ## label encode nominal / categorical features\n",
    "    ## (strictly label encode, not one hot encode) \n",
    "    feat_list_nom = []\n",
    "    nom_dtypes = [\"object\", \"bool\", \"datetime64\"]\n",
    "    \n",
    "    for j in range(d):\n",
    "        if data.dtypes[j] in nom_dtypes:\n",
    "            feat_list_nom.append(j)\n",
    "            data.iloc[:, j] = pd.Categorical(pd.factorize(\n",
    "                data.iloc[:, j])[0])\n",
    "    \n",
    "    data = data.apply(pd.to_numeric)\n",
    "    \n",
    "    ## create numeric feature list\n",
    "    feat_list_num = list(set(feat_list) - set(feat_list_nom))\n",
    "    \n",
    "    ## calculate ranges for numeric / continuous features\n",
    "    ## (includes label encoded features)\n",
    "    feat_ranges = list(np.repeat(1, d))\n",
    "    \n",
    "    if len(feat_list_nom) > 0:\n",
    "        for j in feat_list_num:\n",
    "            feat_ranges[j] = max(data.iloc[:, j]) - min(data.iloc[:, j])\n",
    "    else:\n",
    "        for j in range(d):\n",
    "            feat_ranges[j] = max(data.iloc[:, j]) - min(data.iloc[:, j])\n",
    "    \n",
    "    ## subset feature ranges to include only numeric features\n",
    "    ## (excludes label encoded features)\n",
    "    feat_ranges_num = [feat_ranges[i] for i in feat_list_num]\n",
    "    \n",
    "    ## subset data by either numeric / continuous or nominal / categorical\n",
    "    data_num = data.iloc[:, feat_list_num]\n",
    "    data_nom = data.iloc[:, feat_list_nom]\n",
    "    \n",
    "    ## get number of features for each data type\n",
    "    feat_count_num = len(feat_list_num)\n",
    "    feat_count_nom = len(feat_list_nom)\n",
    "    \n",
    "    ## calculate distance between observations based on data types\n",
    "    ## store results over null distance matrix of n x n\n",
    "    dist_matrix = np.ndarray(shape = (n, n))\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            \n",
    "            ## utilize euclidean distance given that \n",
    "            ## data is all numeric / continuous\n",
    "            if feat_count_nom == 0:\n",
    "                dist_matrix[i][j] = euclidean_dist(\n",
    "                    a = data_num.iloc[i],\n",
    "                    b = data_num.iloc[j],\n",
    "                    d = feat_count_num\n",
    "                )\n",
    "            \n",
    "            ## utilize heom distance given that \n",
    "            ## data contains both numeric / continuous \n",
    "            ## and nominal / categorical\n",
    "            if feat_count_nom > 0 and feat_count_num > 0:\n",
    "                dist_matrix[i][j] = heom_dist(\n",
    "                    \n",
    "                    ## numeric inputs\n",
    "                    a_num = data_num.iloc[i],\n",
    "                    b_num = data_num.iloc[j],\n",
    "                    d_num = feat_count_num,\n",
    "                    ranges_num = feat_ranges_num,\n",
    "                    \n",
    "                    ## nominal inputs\n",
    "                    a_nom = data_nom.iloc[i],\n",
    "                    b_nom = data_nom.iloc[j],\n",
    "                    d_nom = feat_count_nom\n",
    "                )\n",
    "            \n",
    "            ## utilize hamming distance given that \n",
    "            ## data is all nominal / categorical\n",
    "            if feat_count_num == 0:\n",
    "                dist_matrix[i][j] = overlap_dist(\n",
    "                    a = data_nom.iloc[i],\n",
    "                    b = data_nom.iloc[j],\n",
    "                    d = feat_count_nom\n",
    "                )\n",
    "    \n",
    "    ## determine indicies of k nearest neighbors\n",
    "    ## and convert knn index list to matrix\n",
    "    knn_index = [None] * n\n",
    "    \n",
    "    for i in range(n):\n",
    "        knn_index[i] = np.argsort(dist_matrix[i])[1:k + 1]\n",
    "    \n",
    "    knn_matrix = np.array(knn_index)\n",
    "    \n",
    "    ## calculate max distances to determine if gaussian noise is applied\n",
    "    ## (half the median of the distances per observation)\n",
    "    max_dist = [None] * n\n",
    "    \n",
    "    for i in range(n):\n",
    "        max_dist[i] = box_plot_stats(dist_matrix[i])[\"stats\"][2] / 2\n",
    "    \n",
    "    ## number of new synthetic observations for each rare observation\n",
    "    x_synth = int(perc - 1)\n",
    "    \n",
    "    ## total number of new synthetic observations to generate\n",
    "    n_synth = int(n * (perc - 1 - x_synth))\n",
    "    \n",
    "    ## randomly index data by the number of new synthetic observations\n",
    "    r_index = np.random.choice(\n",
    "        a = tuple(range(0, n)), \n",
    "        size = n_synth, \n",
    "        replace = False, \n",
    "        p = None\n",
    "    )\n",
    "    \n",
    "    ## create null matrix to store new synthetic observations\n",
    "    synth_matrix = np.ndarray(shape = ((x_synth * n + n_synth), d))\n",
    "    \n",
    "    if x_synth > 0:\n",
    "        for i in range(n):\n",
    "            \n",
    "            ## determine which cases are 'safe' to interpolate\n",
    "            safe_list = np.where(\n",
    "                dist_matrix[i, knn_matrix[i]] < max_dist[i])[0]\n",
    "            \n",
    "            for j in range(x_synth):\n",
    "                \n",
    "                ## randomly select a k nearest neighbor\n",
    "                neigh = int(np.random.choice(\n",
    "                    a = tuple(range(k)), \n",
    "                    size = 1))\n",
    "                \n",
    "                ## conduct synthetic minority over-sampling\n",
    "                ## technique for regression (smoter)\n",
    "                if neigh in safe_list:\n",
    "                    diffs = data.iloc[\n",
    "                        knn_matrix[i, neigh], 0:(d - 1)] - data.iloc[\n",
    "                        i, 0:(d - 1)]\n",
    "                    synth_matrix[i * x_synth + j, 0:(d - 1)] = data.iloc[\n",
    "                        i, 0:(d - 1)] + rd.random() * diffs\n",
    "                    \n",
    "                    ## randomly assign nominal / categorical features from\n",
    "                    ## observed cases and selected neighbors\n",
    "                    for x in feat_list_nom:\n",
    "                        synth_matrix[i * x_synth + j, x] = [data.iloc[\n",
    "                            knn_matrix[i, neigh], x], data.iloc[\n",
    "                            i, x]][round(rd.random())]\n",
    "                    \n",
    "                    ## generate synthetic y response variable by\n",
    "                    ## inverse distance weighted\n",
    "                    for z in feat_list_num:\n",
    "                        a = abs(data.iloc[i, z] - synth_matrix[\n",
    "                            i * x_synth + j, z]) / feat_ranges[z]\n",
    "                        b = abs(data.iloc[knn_matrix[\n",
    "                            i, neigh], z] - synth_matrix[\n",
    "                            i * x_synth + j, z]) / feat_ranges[z]\n",
    "                    \n",
    "                    if len(feat_list_nom) > 0:\n",
    "                        a = a + sum(data.iloc[\n",
    "                            i, feat_list_nom] != synth_matrix[\n",
    "                            i * x_synth + j, feat_list_nom])\n",
    "                        b = b + sum(data.iloc[knn_matrix[\n",
    "                            i, neigh], feat_list_nom] != synth_matrix[\n",
    "                            i * x_synth + j, feat_list_nom])\n",
    "                    \n",
    "                    if a == b:\n",
    "                        synth_matrix[i * x_synth + j, \n",
    "                            (d - 1)] = data.iloc[i, (d - 1)] + data.iloc[\n",
    "                            knn_matrix[i, neigh], (d - 1)] / 2\n",
    "                    else:\n",
    "                        synth_matrix[i * x_synth + j, \n",
    "                            (d - 1)] = (b * data.iloc[\n",
    "                            i, (d - 1)] + a * data.iloc[\n",
    "                            knn_matrix[i, neigh], (d - 1)]) / (a + b)\n",
    "                    \n",
    "                ## conduct synthetic minority over-sampling technique\n",
    "                ## for regression with the introduction of gaussian \n",
    "                ## noise (smoter-gn)\n",
    "                else:\n",
    "                    if max_dist[i] > 0.02:\n",
    "                        t_pert = 0.02\n",
    "                    else:\n",
    "                        t_pert = max_dist[i]\n",
    "                    \n",
    "                    index_gaus = i * x_synth + j\n",
    "                    \n",
    "                    for x in range(d):\n",
    "                        if pd.isna(data.iloc[i, x]):\n",
    "                            synth_matrix[index_gaus, x] = None\n",
    "                        else:\n",
    "                            synth_matrix[index_gaus, x] = data.iloc[\n",
    "                                i, x] + float(np.random.normal(\n",
    "                                    loc = 0,\n",
    "                                    scale = np.std(data.iloc[:, x]), \n",
    "                                    size = 1) * t_pert)\n",
    "                            \n",
    "                            if x in feat_list_nom:\n",
    "                                if len(data.iloc[:, x].unique() == 1):\n",
    "                                    synth_matrix[\n",
    "                                        index_gaus, x] = data.iloc[0, x]\n",
    "                                else:\n",
    "                                    probs = [None] * len(\n",
    "                                        data.iloc[:, x].unique())\n",
    "                                    \n",
    "                                    for z in range(len(\n",
    "                                        data.iloc[:, x].unique())):\n",
    "                                        probs[z] = len(\n",
    "                                            np.where(data.iloc[\n",
    "                                                :, x] == data.iloc[:, x][z]))\n",
    "                                    \n",
    "                                    synth_matrix[index_gaus, x] = rd.choices(\n",
    "                                        population = data.iloc[:, x].unique(), \n",
    "                                        weights = probs, \n",
    "                                        k = 1)\n",
    "    \n",
    "    if n_synth > 0:\n",
    "        count = 0\n",
    "        \n",
    "        for i in r_index:\n",
    "            \n",
    "            ## determine which cases are 'safe' to interpolate\n",
    "            safe_list = np.where(\n",
    "                dist_matrix[i, knn_matrix[i]] < max_dist[i])[0]\n",
    "            \n",
    "            ## randomly select a k nearest neighbor\n",
    "            neigh = int(np.random.choice(\n",
    "                a = tuple(range(0, k)), \n",
    "                size = 1))\n",
    "            \n",
    "            ## conduct synthetic minority over-sampling \n",
    "            ## technique for regression (smoter)\n",
    "            if neigh in safe_list:\n",
    "                diffs = data.iloc[\n",
    "                    knn_matrix[i, neigh], 0:(d - 1)] - data.iloc[i, 0:(d - 1)]\n",
    "                synth_matrix[x_synth * n + count, 0:(d - 1)] = data.iloc[\n",
    "                    i, 0:(d - 1)] + rd.random() * diffs\n",
    "                \n",
    "                ## randomly assign nominal / categorical features from\n",
    "                ## observed cases and selected neighbors\n",
    "                for x in feat_list_nom:\n",
    "                    synth_matrix[x_synth * n + count, x] = [data.iloc[\n",
    "                        knn_matrix[i, neigh], x], data.iloc[\n",
    "                        i, x]][round(rd.random())]\n",
    "                \n",
    "                ## generate synthetic y response variable by\n",
    "                ## inverse distance weighted\n",
    "                for z in feat_list_num:\n",
    "                    a = abs(data.iloc[i, z] - synth_matrix[\n",
    "                        x_synth * n + count, z]) / feat_ranges[z]\n",
    "                    b = abs(data.iloc[knn_matrix[i, neigh], z] - synth_matrix[\n",
    "                        x_synth * n + count, z]) / feat_ranges[z]\n",
    "                \n",
    "                if len(feat_list_nom) > 0:\n",
    "                    a = a + sum(data.iloc[i, feat_list_nom] != synth_matrix[\n",
    "                        x_synth * n + count, feat_list_nom])\n",
    "                    b = b + sum(data.iloc[\n",
    "                        knn_matrix[i, neigh], feat_list_nom] != synth_matrix[\n",
    "                        x_synth * n + count, feat_list_nom])\n",
    "                \n",
    "                if a == b:\n",
    "                    synth_matrix[x_synth * n + count, (d - 1)] = data.iloc[\n",
    "                        i, (d - 1)] + data.iloc[\n",
    "                        knn_matrix[i, neigh], (d - 1)] / 2\n",
    "                else:\n",
    "                    synth_matrix[x_synth * n + count, (d - 1)] = (b * data.iloc[\n",
    "                        i, (d - 1)] + a * data.iloc[\n",
    "                        knn_matrix[i, neigh], (d - 1)]) / (a + b)\n",
    "                \n",
    "            ## conduct synthetic minority over-sampling technique\n",
    "            ## for regression with the introduction of gaussian \n",
    "            ## noise (smoter-gn)\n",
    "            else:\n",
    "                if max_dist[i] > 0.02:\n",
    "                    t_pert = 0.02\n",
    "                else:\n",
    "                    t_pert = max_dist[i]\n",
    "                \n",
    "                for x in range(d):\n",
    "                    if pd.isna(data.iloc[i, x]):\n",
    "                        synth_matrix[x_synth * n + count, x] = None\n",
    "                    else:\n",
    "                        synth_matrix[x_synth * n + count, x] = data.iloc[\n",
    "                            i, x] + float(np.random.normal(\n",
    "                                loc = 0,\n",
    "                                scale = np.std(data.iloc[:, x]),\n",
    "                                size = 1) * t_pert)\n",
    "                        \n",
    "                        if x in feat_list_nom:\n",
    "                            if len(data.iloc[:, x].unique() == 1):\n",
    "                                synth_matrix[\n",
    "                                    x_synth * n + count, x] = data.iloc[0, x]\n",
    "                            else:\n",
    "                                probs = [None] * len(data.iloc[:, x].unique())\n",
    "                                \n",
    "                                for z in range(len(data.iloc[:, x].unique())):\n",
    "                                    probs[z] = len(np.where(\n",
    "                                        data.iloc[:, x] == data.iloc[:, x][z])\n",
    "                                    )\n",
    "                                \n",
    "                                synth_matrix[\n",
    "                                    x_synth * n + count, x] = rd.choices(\n",
    "                                        population = data.iloc[:, x].unique(), \n",
    "                                        weights = probs, \n",
    "                                        k = 1\n",
    "                                    )\n",
    "            \n",
    "            ## close loop counter\n",
    "            count = count + 1\n",
    "    \n",
    "    ## convert synthetic matrix to dataframe\n",
    "    data_new = pd.DataFrame(synth_matrix)\n",
    "    \n",
    "    ## synthetic data quality check\n",
    "    if sum(data_new.isnull().sum()) > 0:\n",
    "        print(\"oops! synthetic data contains missing values\")\n",
    "    \n",
    "    ## replace label encoded values with original values\n",
    "    for j in feat_list_nom:\n",
    "        code_list = data.iloc[:, j].unique()\n",
    "        cat_list = data_var.iloc[:, j].unique()\n",
    "        \n",
    "        for x in code_list:\n",
    "            data_new.iloc[:, j] = data_new.iloc[:, j].replace(x, cat_list[x])\n",
    "    \n",
    "    ## reintroduce constant features previously removed\n",
    "    if len(feat_const) > 0:\n",
    "        data_new.columns = feat_var\n",
    "        \n",
    "        for j in range(len(feat_const)):\n",
    "            data_new.insert(\n",
    "                loc = int(feat_const[j]),\n",
    "                column = feat_const[j], \n",
    "                value = np.repeat(\n",
    "                    data_orig.iloc[0, feat_const[j]], \n",
    "                    len(synth_matrix))\n",
    "            )\n",
    "    \n",
    "    ## convert negative values to zero in non-negative features\n",
    "    for j in feat_non_neg:\n",
    "        data_new.iloc[:, j][data_new.iloc[:, j] < 0] = 0\n",
    "    \n",
    "    return data_new\n",
    "\n",
    "## euclidean distance calculation\n",
    "def euclidean_dist(a, b, d):\n",
    "    \n",
    "    \"\"\" \n",
    "    calculates the euclidean distance between observations for data \n",
    "    containing only numeric / continuous features, returns float value\n",
    "    \"\"\"\n",
    "    \n",
    "    ## load dependency\n",
    "    # import numpy as np\n",
    "    \n",
    "    ## create list to store distances\n",
    "    dist = [None] * d\n",
    "    \n",
    "    ## loop through columns to calculate euclidean \n",
    "    ## distance for numeric / continuous features\n",
    "    for i in range(d):\n",
    "        \n",
    "        ## the squared difference of values in\n",
    "        ## vectors a and b of equal length \n",
    "        dist[i] = (a.iloc[i] - b.iloc[i]) ** 2\n",
    "        \n",
    "    ## sum all the squared differences and take the square root\n",
    "    dist = np.sqrt(sum(dist))\n",
    "    \n",
    "    ## return distance list\n",
    "    return dist\n",
    "\n",
    "## heom distance calculation\n",
    "def heom_dist(a_num, b_num, d_num, ranges_num, a_nom, b_nom, d_nom):\n",
    "    \n",
    "    \"\"\" \n",
    "    calculates the heterogenous euclidean overlap (heom) distance between \n",
    "    observations for data containing both numeric / continuous and nominal  \n",
    "    / categorical features, returns float value\n",
    "    \n",
    "    ref:\n",
    "        \n",
    "    Wilson, D., Martinez, T. (1997). \n",
    "    Improved Heterogeneous Distance Functions.\n",
    "    Journal of Artificial Intelligence Research, 6:1-34.\n",
    "    https://arxiv.org/pdf/cs/9701101.pdf.\n",
    "    \"\"\"\n",
    "    \n",
    "    ## load dependency\n",
    "    # import numpy as np\n",
    "    \n",
    "    ## create list to store distances\n",
    "    dist = [None] * d_num\n",
    "    \n",
    "    ## specify epsilon\n",
    "    eps = 1e-30\n",
    "    \n",
    "    ## loop through columns to calculate euclidean \n",
    "    ## distance for numeric / continuous features\n",
    "    for i in range(d_num):\n",
    "        \n",
    "        ## epsilon utilized to avoid division by zero\n",
    "        if ranges_num[i] > eps:\n",
    "        \n",
    "            ## the absolute value of the differences between values in\n",
    "            ## vectors a and b of equal length, divided by their range, squared\n",
    "            ## (division by range conducted for normalization)\n",
    "            dist[i] = (abs(a_num.iloc[i] - b_num.iloc[i]) / ranges_num[i]) ** 2\n",
    "    \n",
    "    ## loop through columns to calculate hamming\n",
    "    ## distance for nominal / categorical features\n",
    "    for i in range(d_nom):\n",
    "        \n",
    "        ## distance equals 0 for values that are equal\n",
    "        ## in two vectors a and b of equal length\n",
    "        if a_nom.iloc[i] == b_nom.iloc[i]:\n",
    "            dist[i] = 0\n",
    "        \n",
    "        ## distance equals 1 for values that are not equal\n",
    "        else:\n",
    "            dist[i] = 1\n",
    "        \n",
    "        ## theoretically, hamming differences are squared when utilized\n",
    "        ## within heom distance, however, procedurally not required, \n",
    "        ## as squaring [0,1] returns same result\n",
    "    \n",
    "    ## sum all the squared differences and take the square root\n",
    "    dist = np.sqrt(sum(dist))\n",
    "    \n",
    "    ## return distance\n",
    "    return dist\n",
    "\n",
    "## hamming distance calculation\n",
    "def overlap_dist(a, b, d):\n",
    "    \n",
    "    \"\"\" \n",
    "    calculates the hamming (overlap) distance between observations for data \n",
    "    containing only nominal / categorical features, returns float value\n",
    "    \"\"\"\n",
    "    \n",
    "    ## create list to store distances\n",
    "    dist = [None] * d\n",
    "    \n",
    "    ## loop through columns to calculate hamming\n",
    "    ## distance for nominal / categorical features\n",
    "    for i in range(d):\n",
    "        \n",
    "        ## distance equals 0 for values that are equal\n",
    "        ## in two vectors a and b of equal length\n",
    "        if a.iloc[i] == b.iloc[i]:\n",
    "            dist[i] = 0\n",
    "        \n",
    "        ## distance equals 1 for values that are not equal\n",
    "        else:\n",
    "            dist[i] = 1\n",
    "    \n",
    "    ## sum all the differences   \n",
    "    dist = sum(dist)\n",
    "    \n",
    "    ## return distance\n",
    "    return dist\n",
    "\n",
    "## synthetic minority over / under sampling with gaussian noise for regression\n",
    "def smogn(\n",
    "    \n",
    "    ## primary arguments / inputs\n",
    "    data,                     ## training set  (pandas dataframe)\n",
    "    y,                        ## response variable y by name  (string)\n",
    "    k = 5,                    ## num of neighs for over-sampling  (pos int)\n",
    "    samp_method = \"balance\",  ## % over / under sample  (\"balance\" or extreme\")\n",
    "    drop_na_col = True,       ## auto drop columns with nan's  (bool)\n",
    "    drop_na_row = True,       ## auto drop rows with nan's  (bool)\n",
    "    replace = False,          ## sampling replacement  (bool)\n",
    "    \n",
    "    ## phi relevance function arguments / inputs\n",
    "    rel_thres = 0.5,          ## relevance threshold considered rare  (pos real)\n",
    "    rel_method = \"auto\",      ## relevance method  (\"auto\" or \"manual\")\n",
    "    rel_xtrm_type = \"both\",   ## distribution focus  (\"high\", \"low\", \"both\")\n",
    "    rel_coef = 1.5,           ## coefficient for box plot  (pos real)\n",
    "    rel_ctrl_pts_rg = None    ## input for \"manual\" rel method  (2d array)\n",
    "    \n",
    "    ):\n",
    "    \n",
    "    \"\"\"\n",
    "    the main function, designed to help solve the problem of imbalanced data \n",
    "    for regression, much the same as SMOTE for classification, SMOGN applies \n",
    "    the combintation of under-sampling the majority class (in the case of \n",
    "    regression, values commonly found near the mean of a normal distribution \n",
    "    in the response variable y) and over-sampling the minority class (rare \n",
    "    values in a normal distribution of y, typically found at the tails)\n",
    "    \n",
    "    procedure begins with a series of pre-processing steps, and to ensure no \n",
    "    missing values (nan's), sorts the values in the response variable y by\n",
    "    ascending order, and fits a function 'phi' to y, corresponding phi values \n",
    "    (between 0 and 1) are generated for each value in y, the phi values are \n",
    "    then used to determine if an observation is either normal or rare by the \n",
    "    threshold specified in the argument 'rel_thres' \n",
    "    \n",
    "    normal observations are placed into a majority class subset (normal bin) \n",
    "    and are under-sampled, while rare observations are placed in a seperate \n",
    "    minority class subset (rare bin) where they're over-sampled\n",
    "    \n",
    "    under-sampling is applied by a random sampling from the normal bin based \n",
    "    on a calculated percentage control by the argument 'samp_method', if the \n",
    "    specified input of 'samp_method' is \"balance\", less under-sampling (and \n",
    "    over-sampling) is conducted, and if \"extreme\" is specified more under-\n",
    "    sampling (and over-sampling is conducted)\n",
    "    \n",
    "    over-sampling is applied one of two ways, either synthetic minority over-\n",
    "    sampling technique for regression 'smoter' or 'smoter-gn' which applies a \n",
    "    similar interpolation method to 'smoter', but takes an additional step to\n",
    "    perterb the interpolated values with gaussian noise\n",
    "    \n",
    "    'smoter' is selected when the distance between a given observation and a \n",
    "    selected nearest neighbor is within the maximum threshold (half the median \n",
    "    distance of k nearest neighbors) 'smoter-gn' is selected when a given \n",
    "    observation and a selected nearest neighbor exceeds that same threshold\n",
    "    \n",
    "    both 'smoter' and 'smoter-gn' are only applied to numeric / continuous \n",
    "    features, synthetic values found in nominal / categorical features, are \n",
    "    generated by randomly selecting observed values found within their \n",
    "    respective feature\n",
    "    \n",
    "    procedure concludes by post-processing and returns a modified pandas data\n",
    "    frame containing under-sampled and over-sampled (synthetic) observations, \n",
    "    the distribution of the response variable y should more appropriately \n",
    "    reflect the minority class areas of interest in y that are under-\n",
    "    represented in the original training set\n",
    "    \n",
    "    ref:\n",
    "    \n",
    "    Branco, P., Torgo, L., Ribeiro, R. (2017).\n",
    "    SMOGN: A Pre-Processing Approach for Imbalanced Regression.\n",
    "    Proceedings of Machine Learning Research, 74:36-50.\n",
    "    http://proceedings.mlr.press/v74/branco17a/branco17a.pdf.\n",
    "    \"\"\"\n",
    "    \n",
    "    ## load dependencies\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import random as rd\n",
    "    import bisect as bs\n",
    "    \n",
    "    ## pre-process missing values\n",
    "    if bool(drop_na_col) == True:\n",
    "        data = data.dropna(axis = 1)  ## drop columns with nan's\n",
    "    \n",
    "    if bool(drop_na_row) == True:\n",
    "        data = data.dropna(axis = 0)  ## drop rows with nan's\n",
    "    \n",
    "    ## data quality check\n",
    "    if data.isnull().values.any():\n",
    "        print(\"cannot proceed: data cannot contain NaN values\")\n",
    "    \n",
    "    ## relative threshold parameter quality check\n",
    "    if rel_thres == None:\n",
    "        print(\"cannot proceed: phi relevance threshold required\")\n",
    "    \n",
    "    ## input quality check for k number specification\n",
    "    if k > len(data):\n",
    "        print(\"cannot proceed: k is greater than number of \\\n",
    "               observations / rows contained in the dataframe\")\n",
    "    \n",
    "    ## store data dimensions\n",
    "    n = len(data)\n",
    "    d = len(data.columns)\n",
    "    \n",
    "    ## store original data types\n",
    "    feat_dtypes_orig = [None] * d\n",
    "    \n",
    "    for j in range(d):\n",
    "        feat_dtypes_orig[j] = data.iloc[:, j].dtype\n",
    "    \n",
    "    ## determine column position for response variable y\n",
    "    y_col = data.columns.get_loc(y)\n",
    "    \n",
    "    ## move response variable y to last column\n",
    "    if y_col < d - 1:\n",
    "        cols = list(range(d))\n",
    "        cols[y_col], cols[d - 1] = cols[d - 1], cols[y_col]\n",
    "        data = data[data.columns[cols]]\n",
    "    \n",
    "    ## store original feature headers and\n",
    "    ## encode feature headers to index position\n",
    "    feat_names = list(data.columns)\n",
    "    data.columns = range(d)\n",
    "    \n",
    "    ## sort response variable y by ascending order\n",
    "    y = pd.DataFrame(data[d - 1])\n",
    "    y_sort = y.sort_values(by = d - 1)\n",
    "    y_sort = y_sort[d - 1]\n",
    "    \n",
    "    ## -------------------------------- phi --------------------------------- ##\n",
    "    ## calculate parameters for phi relevance function\n",
    "    ## (see 'phi_ctrl_pts()' function for details)\n",
    "    phi_params = phi_ctrl_pts(\n",
    "        \n",
    "        y = y_sort,                ## y (ascending)\n",
    "        method = rel_method,       ## defaults \"auto\" \n",
    "        xtrm_type = rel_xtrm_type, ## defaults \"both\"\n",
    "        coef = rel_coef            ## defaults 1.5\n",
    "    )\n",
    "    \n",
    "    ## calculate the phi relevance function\n",
    "    ## (see 'phi()' function for details)\n",
    "    y_phi = phi(\n",
    "        \n",
    "        y = y_sort,                ## y (ascending)\n",
    "        ctrl_pts = phi_params      ## from 'phi_ctrl_pts()'\n",
    "    )\n",
    "    \n",
    "    ## phi relevance quality check\n",
    "    if all(i == 0 for i in y_phi):\n",
    "        print(\"redefine phi relevance function: all points are 1\")\n",
    "    \n",
    "    if all(i == 1 for i in y_phi):\n",
    "        print(\"redefine phi relevance function: all points are 0\")\n",
    "    ## ---------------------------------------------------------------------- ##\n",
    "    \n",
    "    ## determine bin (rare or normal) by bump classification\n",
    "    bumps = [0]\n",
    "    \n",
    "    for i in range(0, len(y_sort) - 1):\n",
    "        if ((y_phi[i] >= rel_thres and y_phi[i + 1] < rel_thres) or \n",
    "            (y_phi[i] < rel_thres and y_phi[i + 1] >= rel_thres)):\n",
    "            bumps.append(i + 1)\n",
    "    \n",
    "    bumps.append(n)\n",
    "            \n",
    "    ## number of bump classes\n",
    "    n_bumps = len(bumps) - 1\n",
    "    \n",
    "    ## determine indicies for each bump classification\n",
    "    b_index = {}\n",
    "    \n",
    "    for i in range(n_bumps):\n",
    "        b_index.update({i: y_sort[bumps[i]:bumps[i + 1]]})\n",
    "    \n",
    "    ## calculate over / under sampling percentage according to\n",
    "    ## bump class and user specified method (\"balance\" or \"extreme\")\n",
    "    b = round(n / n_bumps)\n",
    "    s_perc = []\n",
    "    scale = []\n",
    "    obj = []\n",
    "    \n",
    "    if samp_method == \"balance\":\n",
    "        for i in b_index:\n",
    "            s_perc.append(b / len(b_index[i]))\n",
    "            \n",
    "    if samp_method == \"extreme\":\n",
    "        for i in b_index:\n",
    "            scale.append(b ** 2 / len(b_index[i]))\n",
    "        scale = n_bumps * b / sum(scale)\n",
    "        \n",
    "        for i in b_index:\n",
    "            obj.append(round(b ** 2 / len(b_index[i]) * scale, 2))\n",
    "            s_perc.append(round(obj[i] / len(b_index[i]), 1))\n",
    "    \n",
    "    ## conduct over / under sampling and store modified training set\n",
    "    data_new = pd.DataFrame()\n",
    "    \n",
    "    for i in range(n_bumps):\n",
    "        \n",
    "        ## no sampling\n",
    "        if s_perc[i] == 1:\n",
    "            \n",
    "            ## simply return no sampling\n",
    "            ## results to modified training set\n",
    "            data_new = pd.concat([data.iloc[b_index[i].index], data_new])\n",
    "        \n",
    "        ## over-sampling\n",
    "        if s_perc[i] > 1:\n",
    "            \n",
    "            ## generate synthetic observations\n",
    "            ## (see 'synth_gen()' function for details)\n",
    "            synth_obs = synth_gen(\n",
    "                data = data,\n",
    "                index = list(b_index[i].index),\n",
    "                perc = s_perc[i],\n",
    "                k = k\n",
    "            )\n",
    "            \n",
    "            ## concatenate over-sampling\n",
    "            ## results to modified training set\n",
    "            data_new = pd.concat([synth_obs, data_new])\n",
    "        \n",
    "        ## under-sampling\n",
    "        if s_perc[i] < 1:\n",
    "            \n",
    "            ## drop observations in training set\n",
    "            ## considered 'normal' (not 'rare')\n",
    "            omit_index = np.random.choice(\n",
    "                a = list(b_index[i].index), \n",
    "                size = int(s_perc[i] * len(b_index[i])),\n",
    "                replace = replace\n",
    "            )\n",
    "            \n",
    "            omit_obs = data.drop(data.iloc[omit_index], axis = 0)\n",
    "            \n",
    "            ## concatenate under-sampling\n",
    "            ## results to modified training set\n",
    "            data_new = pd.concat([omit_obs, data_new])\n",
    "    \n",
    "    ## rename feature headers to originals\n",
    "    data_new.columns = feat_names\n",
    "    \n",
    "    ## restore response variable y to original position\n",
    "    if y_col < d - 1:\n",
    "        cols = list(range(d))\n",
    "        cols[y_col], cols[d - 1] = cols[d - 1], cols[y_col]\n",
    "        data_new = data_new[data_new.columns[cols]]\n",
    "    \n",
    "    ## restore original data types\n",
    "    for j in range(d):\n",
    "        data_new.iloc[:, j] = data_new.iloc[:, j].astype(feat_dtypes_orig[j])\n",
    "    \n",
    "    ## return modified training set\n",
    "    return data_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162fb368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use SMOGN\n",
    "X_smogn = smogn(\n",
    "    ## primary arguments / inputs\n",
    "    D_Origin,                     ## training set  (pandas dataframe)\n",
    "    key,                        ## response variable y by name  (string)\n",
    "    k = 2,                    ## num of neighs for over-sampling  (pos int)\n",
    "    samp_method = \"balance\",  ## % over / under sample  (\"balance\" or extreme\")\n",
    "    drop_na_col = True,       ## auto drop columns with nan's  (bool)\n",
    "    drop_na_row = True,       ## auto drop rows with nan's  (bool)\n",
    "    replace = False,          ## sampling replacement  (bool)\n",
    "    \n",
    "    ## phi relevance function arguments / inputs\n",
    "    rel_thres = 0.6,          ## relevance threshold considered rare  (pos real)\n",
    "    rel_method = \"auto\",      ## relevance method  (\"auto\" or \"manual\")\n",
    "    rel_xtrm_type = \"low\",   ## distribution focus  (\"high\", \"low\", \"both\")\n",
    "    rel_coef = 0.5,           ## coefficient for box plot  (pos real)\n",
    "    rel_ctrl_pts_rg = None    ## input for \"manual\" rel method  (2d array)\n",
    "    )\n",
    "X_smogn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e8d117",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_smogn = pd.concat([X_smogn, D_Origin])\n",
    "cleaned_smogn = concat_smogn.drop_duplicates()\n",
    "cleaned_smogn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df04406b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_smogn.to_csv(\"SMOGN.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d23296",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = cleaned_smogn.drop(columns=[key])\n",
    "y_train = cleaned_smogn[[key]]\n",
    "\n",
    "lgbm = lgb.LGBMRegressor()\n",
    "lgbm.fit(X_train, y_train.values.ravel())\n",
    "y_pred = lgbm.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab63c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the prediction scores\n",
    "print('RMSE: {}'.format(np.sqrt(mean_squared_error(y_test, y_pred))))\n",
    "print('MAE: {}'.format(mean_absolute_error(y_test, y_pred)))\n",
    "print('R-squared: {}'.format(r2_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f44df47",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_pred, y_test)\n",
    "plt.plot(np.linspace(0,1000,1000), np.linspace(0,1000,1000), c = 'orange', linestyle='--')\n",
    "plt.xlabel('prediction')\n",
    "plt.ylabel('true values')\n",
    "plt.xlim(0,1000)\n",
    "plt.ylim(0,1000)\n",
    "plt.title('Predicted vs True values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21d0bcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ee3d57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f02fb92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
